
1 Aug 2021

Probably the simplest route to convolutional layers is to give each
node a start_weight_index, which is where we start reading the
indices_per_node weights for the node. For sparse and dense layers,
this is like node_idx * indices_per_node since all weights are
independent. Then we assign the indices using whatever regular
structure we want (e.g. m*n box around the pixel), and since this
now generalizes sparse, dense and convolutional layers, we can
use the same kernel to predict and train.

Two bummers here: The indices are not stored efficiently; they could
(probably--edge conditions?) just be computed from the index of the
node. Worse, for both indices and weights, we have an extra indirection
which creates a data-dependent load (like
weights[start_weight_index[node] + i]). So it would probably be better
to have convolution-specific kernels. Weights seems to be the easier
problem here because the weights are just like
weights[feature_number * indices_per_node + i]
whereas the input of the convolution might not be so easy to describe,
especially if we do something fancy on the boundary.



Other idea: We probably don't want to just have pure convolutional
layers with the same indices_per_node throughout. As a simple example,
in this problem we expect to have a short vector of "plugin
parameters" alongside the wave data, and although we do want to
convolve the plugin data, it would be weird to convolve the
parameters, especially to treat them as though they are just samples.
With the approach described above, we could make the parameters be
arguments to each convolution (just by setting the indices), which
works cleanly and seems to make sense. We might also want to have a
portion of the layer be some dense combination of parameters, or a
verbatim copy of the input. So idea two is: Make each "layer" actually
a collection of Chunks (which are probably what are now called
Layers). Each chunk can have different settings (i.e. type, ipn,
transfer function...) so that we can mix convolutional and sparse, or
sparse with different IPN. Chunks should have independent parameters
(weights, biases) but could overlap in what input nodes they read.
This could cause some complication when propagating errors back, but
it seems manageable (errors are just summed, right?).

(Indeed: Looks like we can just run BackwardsLayer for each chunk,
then sum, and probably apply clipping at the end.)



Experimenting with the above a little, I think it's actually bad;
you still have a lot of indices (e.g. one NES frame is 256x240x3,
with 8x8 convs, 11.8M indices), and you still need to invert them
somehow (unclear how to do this efficiently)? So now I'm thinking
that since the indices follow a regular pattern, it's just better
to generate them in code. We are generating the CL code anyway,
so this ought to be pretty efficient!

Goal is to generate a set of source/input node ids from a dest node id
on the next layer (we'll also need to be able to convert the inverse
of this function). (Thinking forward to "Chunks", the dest id will be
local to the chunk, but the source should be an index in the layer.)

Some terms:
  The convolution array layer is an array of features (e.g., image
  features). The features all share a node pattern (e.g. an 8x8
  square) that is repeated at different offsets. Each repetition is
  an occurrence, and each feature's occurrence is 1:1 with an output
  node. The output nodes are interlaved, so that all the features
  for an occurrence are adjacent.

  The indices for each feature are the same, but each feature has its
  own independent weight vector. A feature's weights are the same for
  each of its occurrences. So there are num_features * indices_per_node
  total weights for the layer.

So we have something like this:

   occurrence_x_stride
   -------->
  [o1][  ][o2][  ][  ][  ] ...   <- src layer
  [  ][  ][  ][  ][  ][  ] ...
  [  ][  ][  ][  ][  ][  ] ...
  [  ][  ][  ][  ][  ][  ] ...

   d0  d1  d2  d3  d4  d5 ...
  [f1][f2][f3][f1][f2][f3]   <- dst layer
  |occ1------||occ2------|


where occ1 is the first occurrence of the pattern, with three features
in it, f1, f2, f3. o1 is the offset of occurrence 1. o2 is the offset
of occurrence 2; it is not immediately after o1 because an
occurrence_x_stride=2 (for example, if the src layer is two-channel).

Layer constants:
 num_features
 pattern_width
 pattern_height
 (indices_per_node = pattern_width * pattern_height)
 src_width
 src_height
 (src_num_nodes = src_width * src_height)
 occurrence_x_stride
 num_occurrences_across (see below)
// XXX want occurrence_y_stride too

src_width, src_height are how this convolution interprets the previous
layer, which need not be the same as the presentational
width/height/channels. (In fact, a normal choice for src_width would
be width * channels).

// The destination index from the flat array of output nodes.
// (Input to the kernel etc.)
int dst_idx;
// The occurrence number and feature number come directly
// from the interleaving.
int occ = dst_idx / num_features;
int feature = dst_idx % num_features;

The indices are all a property of the occurrence, since they are shared
by all the features.

Now, the starting offset for the occurrence. We need to know how many
patterns fit "across" the src_width. Two complexities:
  - Since the pattern is typically wider than one pixel, we won't
    fit src_width of them (we don't try to read outside).
  - We space the pattern occurrences according to occurrence_x_stride.

It would be nice if there were a closed form for num_occurrences_across
(with x_stride = 1 it is (src_width - pattern_width + 1) but it gets
complicated with the stride when it doesn't end up evenly dividing).
but this is just a constant so we can compute it once, iteratively
(see cont_test.cc). We want it to be a constant anyway because then
we do integer division/modulus:

int occ_row = occ / num_occurrences_across;
int occ_col = occ % num_occurrences_across;

... the position of the occurrence in the (conceptual) packed output
layer. But these are just used to derive the input coordinates:

int src_row = occ_row * occurrence_y_stride;
int src_col = occ_col * occurrence_x_stride;

int src_start_offset = src_row * src_width + src_col;


// Finally, the loop over the pattern. There is no "stride" here;
// we always densely look at adjacent nodes.
// We should definitely benchmark unrolling loops here, since
// these patterns are usually like 8x8 (or trust the compiler..)

for (int py = 0; py < pattern_height; py++) {
  // Always the adjacent row.
  int src_offset = src_start_offset + (py * src_width);
  for (int px = 0; px < pattern_width; px++) {
    emit(src_offset)
    // Always the adjacent node.
    src_offset++;
  }
}


NOW, how do we reverse this (for the backwardlayer pass)? It is not
trivial, for one thing because nodes in the source layer can have
a different number of destinations, due to edge conditions (e.g. the
top-left node is only in ONE occurrence).

Some options:
 - Treat it as an edgeless problem, but skip nodes that are out-of-bounds
   when enumerating them. Could work? Not great to have branches in the
   inner loop, though.
 - The backwards pass is currently implemented as a loop over the nodes
   of the source layer, where we sum up all the errors from nodes that
   we output to, modulated by the weights. There are other ways to
   compute this, for example, looping over the dest layer and accumulating
   the weighted error into source nodes. The reason we don't do this
   normally is so that the pass can be done in parallel; we don't want two
   writes (in the +=) to conflict.
   We could maybe partition into non-overlapping zones and run them in
   parallel? And then sum? This seems plausible because of the regular
   structure. Like you might be able to decompose into
   |pattern| layers that are known to be disjoint, and even sum those
   in parallel after. Could compute this partioning up front.
   (But note that if we're willing to have a memory overhead of
    |pattern| * num_nodes PER TRAINING EXAMPLE, why not just store the
    inverted indices once, which are |pattern| * num_nodes per network?
    So, you could keep summing into an existing array, but following
    a schedule where you know that each pass can be done in parallel conflict.
    Here you'd have |pattern| passes, but each one would be 1/|pattern| as
    big as normal.)
   (Still, there are cases (e.g. stride = pattern_width) where overlap
    is minimal or non-existent, and then this would be an efficient
    way to go. Of course, these cases are also faster in the inverted
    index approach.)

 - Just generate a flattened inverted-indices array like we currently
   do for sparse layers, using the formulae above. Downside here is that
   it may be costly to store these explicitly, and we have to do all
   these data-dependent indirect reads. But it is a good way to get there
   incrementally...



--------------------------------------------------
19 Sep 2021

Chunks!

Currently a layer can only be one type (dense, convolution array, etc.).
Goal is to permit multiple different types per layer. Call
these chunks.

[.....stimulation.......]

[chunk 1][chunk       2][chunk3]   = layer
|       ||             ||      |
[...... output stimulation ....]

each chunk is an array of nodes with a type and some other
configuration, like a layer is today. The size of the output
of that layer is the sum of the sizes of its constituent
chunks. Each of these chunks can read from the entire input
layer. We expect a small number of chunks per layer, so we're
not too concerned with overhead of them, but we don't want
to pay a penalty for the inner loops.

For now, we'll think about the "compiled-down" representation of the
network that's suitable for training and inference. We probably
want to rethink how the networks are described at a higher level,
in like new-network and widen, etc.

In the forward direction, we compile a kernel for each chunk of
each layer, baking in some constants as usual. Probably we
generalize DENSE to include a source offset and size, to allow
for something like a dense column parallel to some convolution.
Since a convolution also treats the input as a 2D matrix, it
also needs to designate a region of the input. A linear region
should suffice.
We should pass an input pointer or use global_work_offset to
avoid adding the offset inside the kernel itself, even as a
compile-time constant.
Sparse layers may not need offset/size, but it is not harmful there,
so probably this is a property of all chunks.


Since each chunk's outputs are independent, the forward pass is all
pretty easy; for the chunk's output pointer we just pass the
corresponding part of the output stimulation. We can even run the
kernels in parallel, as their reads may overlap but their writes
will not.


setoutputerror needs no changes, as it is just working with the
last stimulation and the expected part of the training example.

decayweights does not need changes if we pack the weights for
each layer into an array. But I'm thinking that we actually want
each layer to be represented as an array of chunks with the
weights/indices separated? This will be mostly an issue for
backwardlayer/updatweights.

Backwardlayer is a little interesting, at least, since multiple chunks
may read from the same input node, and thus their total error cannot
be computed chunk-by-chunk in a single pass.

For backwardlayer, the simplest thing is for each chunk to have
its own inverted indices. Perhaps the very easiest thing is
to start with error of 0, then iteratively compute error for
each of the N chunks, += into the error array (and then
clipping as a separate pass at the end, if enabled). We can use
define tricks so that we don't need the 0 += v on the first chunk of
the layer. (Or as an additional optimization, we can order the chunks
such that the first K >= 1 of them have disjoint sources, and enable
this for all of them.)

So backwardlayer isn't a loop over all the nodes of the source layer.
Rather:
  * initialize error for entire source layer to 0
  * loop over each chunk in the dest layer
     = loop over its input span, computing an error increment for that
       span (but the first chunk is just =, not 0) and writing it to
       the full layer's error
     = here, the inverted indices are specific to the chunk (in the
       dest layer, because we don't even want to store them for dense
       chunks)...
         - indexed by chunk offset (e.g. elt 0 is source node span_start).
         - contents are just the uses that fall into this chunk; it's
           no longer globally comprehensive
         - need to decide whether the nodes are given as chunk-local indices
           or global; should just do what's most efficient.
  * post-pass clipping if enabled


Alternatively, we could (in parallel) compute the error to N arrays of
the size of the source layer (or be a little smarter and only
represent the part covered by the offset/size) and then sum them.
This significantly intermediate memory use though.

In any case, backwardlayer requires some work but is straightforward.


For updateweights, we can do this independently for each chunk, even
in parallel. We just need to use the right offsets to read the error
and outputs. So this one (which is hairy for convolutional layers)
is mercifully easy.


Overall this does not seem too bad, just a little fiddly?



--------------------------------------------------

OK, so that's implemented, but now testing. Really basic stuff works ok.

I'm unable to learn the 2^(2^3) functions on three variables, which
seems surprising and might be indicative of a problem. (Problem might be
that the learning rate or other hyperparameters are off, though, sigh.)

One sign of an actual bug:
 - the second to last layer has a strange pattern of weights:
   it's 256 nodes wide, with input span of size 64+6, sparse
   there's a very distinct difference between the weights on
   the first 64 nodes (reasonable range) and the remaining 192 (saturated)

 so it could be that we have a bug here where one of the backprop steps
 is not working correctly (like, it's using the wrong work size, perhaps
 reading the size from the wrong layer.) 64 is of course suspiciously
 round. It's the size of the second chunk in the previous layer, and
 also the size of the examples_per_round!


 - this happens by round 5000. (actually 1000 seems to be enough)
 - this persists at 64 nodes even when:
     - I reverse the sense of the boolean functions (~)
     - I permute the boolean function randomly
     - I change the examples_per_round to 54
     - I change the size of the chunk on the previous layer from 64 to 84??

 Uhhh so this could be a byte size thing? 256/4 = 64?
 Aha yes! If I switch this to 200 nodes, then the region is size 50.
 Yes, that was it! The call to clEnqueueFillBuffer was only filling
 buffer/4 with zeroes, so I guess we would just keep accumulating
 error for the remaining 3/4.

 And now it converges to 0 incorrect in like 900 rounds! :)




--------------------------------------------------

learning rates

"Adam" is a popular approach. It seems pretty simple, although one
downside is that it adds two float parameters per weight that we need
to retain and access during training, so this triples our memory
overhead for the model itself. It also adds two hyper-parameters
like the learning rate, but they are reasonably easy to understand.


These parameters are the first and second moments of the gradient X (a
random variable). Moment k just means the expected value of X^k, so
the first moment is the mean, and the second moment is the uncentered
variance (uncentered = don't subtract the mean first). These are
typically 'm' and 'v' in the presentation. Each one is updated by an
exponential moving average

m = B1 * m + (1 - B1) * g
v = B2 * v + (1 - B2) * g^2

... where B1 and B2 are the hyperparameters, and g is the gradient.

m and v for each parameter is 0 at the beginning, and
B1 = 0.9 and B2 = 0.999 seem to be solid choices.

Since we start with zeroes, these estimates are biased towards zero
(although that error decays away exponentially). Still the Algorithm
corrects for this:

m' = m / (1 - B1^t)
v' = v / (1 - B2^t)

where t is the time step. I wonder if this can actually be skipped, as
it will be basically dividing by 1 after a few hundred rounds. Maybe
we can efficiently turn it off by recompiling kernels when it is
negligible.

Finally, we use these to update the weights.
w = w - r * (m' / (sqrt(v') + e))

With SGD the update would just be 'r * g', but here we are first
basically averaging over recent updates (for m') and then dividing
by the (normalized) square... what's the intuition here? When the update
is small AND the variance is small, we take larger steps (that makes
sense I guess?) ... but when updates are large with low variance,
because it's uncentered variance, we still end up dividing by something
 > 1, so we take smaller steps than SGD?

(I think we actually use positive weight updates, not - errors, so
this sign may be flipped. v' is computed from squares, so it's always
non-negative.)

r is the round learning rate, as usual
e is epsilon to prevent dividing by zero (e.g. 1e-8. Or treat as a
hyperparameter...)

Anyway, this is all pretty easy to implement; let's do...



--------------------------------------------------

learn-words

fixed bug at round 17530, oops

--------------------------------------------------

bug with weight updates?
copy chunks are fixed so they should never get updates.

(it happened again! the network just detonated near round 515000:
514234	0	f	162.908935547
516234	0	f	903020.75
... and the copy layers have red and green pixels in 'em.
they should never be updated!)

dumping the weights from e.g. chunk 4.0 shows that lots of the
weights are -16 or 16, so these are very likely the result of
weight updates. Perhaps notable that both weights and biases
on this layer have larger absolute values towards the beginning?
5.0 exhibits that for biases but not so much for weights.
6.0 is like 5.0.

so simple -- I just wasn't serializing this field, so if I ever
restarted the program, it would get lost.

I don't think this can really explain the "detonation" behavior,
but it's at least wrong, so let's try again?


1284800: 0.014<2.867<6.105 | 0<2.563<6 (15490.48 eps)
   [it has been (proposed) that a] got [it has been (said) that a]

very good :)

With 2048 words, it never makes progress (probably always predicting
zero everywhere because loss is exactly 6), even after >24h and even
with fairly aggressive learning rates. Could be numerical underflow? I
think all I changed here was constants.
