
\documentclass[twocolumn]{article}
% {amsart}
\usepackage[top=0.75in, left=0.65in, right=0.65in, bottom=0.6in]{geometry}

\usepackage[utf8]{inputenc}

\usepackage{float}

% \usepackage{url}
% \usepackage{xurl}
\usepackage[pdftex]{hyperref}

% code sux, use "lstlisting"
%\usepackage{code}
\usepackage{listings}
% \usepackage{cite}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
% IPA symbols. safe turns off overrides for like \! which I still want
% \usepackage[safe]{tipa}
% \usepackage{chessboard}
\usepackage{supertabular}
\usepackage{enumitem}

% \usepackage[most]{tcolorbox}

% lets me explicitly set a. or 1. etc. as enum label
% \usepackage{enumitem}

\pagestyle{empty}

\usepackage{ulem}
% go back to italics for emphasis, though
\normalem

% do "fancy" stuff like verbatim in footnotes
\usepackage{fancyvrb}
\VerbatimFootnotes

\usepackage{natbib}
\setcitestyle{numbers,square}

\interfootnotelinepenalty=0
\setlength{\footnotesep}{2em}

% \startsquarepar and \stopsquarepar allows justifying the
% whole paragraph, including its last line
\newcommand{\startsquarepar}{%
    \par\begingroup \parfillskip 0pt \relax}
\newcommand{\stopsquarepar}{%
    \par\endgroup}

% Style for code listings.
\lstdefinestyle{codestyle}{
  basicstyle=\ttfamily\footnotesize,
  breaklines=false,
  keepspaces=true,
}

\lstset{style=codestyle}

\newcommand\sfrac[2]{\!{}\,^{#1}\!/{}\!_{#2}}
\newcommand\xbyx[2]{\ensuremath{#1 {\times} #2}}
\newcommand\basis{\ensuremath{\textrm{\bf b}}}

\newcommand\comment[1]{}

% XXX the collection of crap above causes a first page
% with just "1" on it (probably conflicting packages). This
% magic just drops the first page, which is of course not
% the right solution, but...

\usepackage{atbegshi}% http://ctan.org/pkg/atbegshi
\AtBeginDocument{\AtBeginShipoutNext{\AtBeginShipoutDiscard}}

\begin{document}

\title{GradIEEEnt half decent}
\author{Dr.~Tom~Murphy~VII~Ph.D.}\thanks{
Copyright \copyright\ 2023 the Regents of the Wikiplia Foundation.
Appears in SIGBOVIK~2023 with the
signaling NaN
of the Association for Computational Heresy; {\em IEEEEEE!}
press, Verlag-Verlag volume no.~0x40-2A. 1 ULP
}

\renewcommand\th{\ensuremath{{}^{\textrm{th}}}}
\newcommand\st{\ensuremath{{}^{\textrm{st}}}}
\newcommand\rd{\ensuremath{{}^{\textrm{rd}}}}
\newcommand\nd{\ensuremath{{}^{\textrm{nd}}}}

\newcommand\gradone{{\sf grad1}}
\newcommand\downshifttwo{{\sf downshift2}}
\newcommand\plussixtyfour{{\sf plus64}}

\renewcommand\paragraph[1]{\smallskip \noindent{\bf #1}\enspace}

\date{X April 2023}

\maketitle \thispagestyle{empty}

\sloppypar


\section{Introduction}



Imagine you are my professor. Maybe you actually were my professor, in
which case you may already be sweating before I say any more. The
subject matter is Neural Networks. You draw the

``Now this transfer function can be almost anything. Typically it
would be something like the hyperbolic tangent, which looks like this.

\begin{center}
\includegraphics[width=0.75 \linewidth]{tanh}
\end{center}

``But it has to be a non-linear function. If it's linear, i.e.~of the
form $y = mx + b$, then observe that the entire layer is a linear
function. And so the entire network is just a linear function of
linear functions; itself a linear function. We could just compute an
equivalent single-layer network, and we know that it could only fit
linear functions, which is insufficient for most problems.''

Then I raise my hand. The speed with which I raise it, and the subtle
forward pose of my arm suggests that I want to pluck an abstract idea
from the whiteboard and pervert it. You know this look, and you're
reluctant to call on me. But no other students are asking questions.
You must call on me.

``Tom.'' It's more like a statement then a question. It includes the
tone of spoken punctuation that, if it could, ends the entire
conversation before it begins.

``OK but, when we implement this on a computer we'll use some
approximation of numbers, like floating point. So the specific
sequence of additions and multiplications will matter. It's not
actually equivalent to rearrange them to a single layer because you
don't have distributivity, commutativity, etc.''

``Uh. I think that's technically true, but for all practical purposes \ldots''

``What about *im*practical purposes?''

You vigorously strangle me, and I die.

$\bigskip$
$\cdots$
$\bigskip$

That was about 20 years ago. The world will not let us stop thinking
about neural networks. And so this question has been on my mind for a
long time. Just to be clear, the professor was right: This is not an
important question. Theoretically I am right, but for practical
purposes it probably does not matter. But I like to work at the
intersection of Theory and Impractice. We can make it matter by doing
a lot of work. And then I will continue to be right theoretically, but
also more right because it will only matter for {\em most} practical
purposes.

So this paper is an exhaustive exploration of what we can do with just
floating point addition and multiplication by constants (scaling). You
should only be able to make lines, but I'll demonstrate that due to
rounding error, you can absolutely use ``linear'' transfer functions
in neural networks. Machine learning is not the only field with a
proclamation that some function must be ``non-linear,'' so we'll look
at a few of those as well. There will of course be several hearty
digressions. By studying these functions we'll see that they are
almost arbitrarily rich, and conclude with a demonstration of their
completeness in the field of Plumbing.

TODO intro: We'll develop a better understanding of where floating point inaccuracy comes from, and how to get it quickly, and how to use it for our benefit.

\section{A refresher on neural networks}
Let's repeat the professor's lesson. This section is easily skippable
if you are a plucky student who thinks they already know everything.
At a high level, a NN is a way of implementing a numeric function
(takes a bunch of numbers as input, and gives a bunch of numbers as
output). The network consists of a number of {\it layers}, where the first
layer is the input and the last layer is the output.

TODO DIAGRAM

The numbers that fill in each layer are its {\it activations}. Each
layer's activations are computed from (just) the previous layer,
according to a formula:

$$a_i = TF(w_{1i} * p_1 + \ldots w_{ni} * p_n + b_i)$$

Where $p_j$ is the activation of the $j$th node on the previous layer,
and $w_{ji}$ is the weight parameter, and $b_i$ is the bias parameter.
The weight and bias parameters are learned during the training of the
neural network, but just become constants when using the NN to compute
its output.

$TF$ is the transfer function, which is of particular interest in this
project. Classically, the transfer function was some kind of sigmoid.
The {\tt tanh} function pictured in the introduction is a good example
of a sigmoid. The intuition behind this is that, thinking about $a_i$
as some kind of neuron, the neuron ``fires'' (activates) with some
probability. This probability gets higher as its input values get
larger (but note that weights can be negative, so upstream neurons can
also have an inhibitory effect). But since it is a probability, the
% XXX ok but tanh actually outputs [-1,1]
value must be in [0, 1]; the transfer function ensures this.

\paragraph{Differentiability.}
Another important property of the transfer function is that it be
differentiable, because the stochastic gradient descent algorithm used
to train NNs needs to be able to move along some error-reducing
gradient, and back-propagate errors to earlier layers. This gradient
is just the derivative of the function.

\paragraph{What transfer functions ought to exist?}
We used to think that these saturating transfer functions were ideal.
But this turns out to be wrong, especially for internal (``hidden'')
layers. Transfer functions don't need to produce probabilities, and
they can have unbounded range. A wide variety of functions will work,
including extremely simple ones. The most popular transfer function in
2023 is the ``rectified linear unit,'' which looks like this:

\begin{center}
\includegraphics[width=0.33 \linewidth]{relu}
\end{center}

This one is extremely easy to implement (\verb+x < 0 ? 0 : x+), is fast
and seems to work very well, possibly because its derivative is
significant (one) on the entire positive side. (In contrast, sigmoids
tend to get ``stuck'' because of their saturating behavior; their
derivatives become nearly zero.) Note that it is not actually
differentiable (discontinuity at zero) but ``for all practical
purposes'' it is differentiable.

The (only?) apparently essential quality of the transfer function is
that it be non-linear. If it is instead of the form $TF(x) = mx + b$,
then $a_i$ is also just a linear function of the previous layer, as
linear functions of linear functions (weighted sum) are linear. This
causes the entire NN to be a linear function. It is well known that a
linear function ``cannot'' represent some other simple functions, such
as XOR.

$$\nexists m,n,b.\,\, XOR(x, y) \approxeq m x + n y + b$$
% FORMULA: Not exists m n b, such that $XOR(x, y) \approxeq m x + n y + b$

This means that with a linear transfer function, a NN could never
learn even a simple function like XOR. Many problems we want to learn
are in fact much more complicated.

\section{A fine terminological issue}
affine vs linear

(Here we need to clarify exactly what operations we allow!) THE RULES


\section{Half-precision IEEE-754 floating point}
In this project we'll abuse floating point inaccuracy to create
``linear'' functions (only using floating point addition and scaling)
that are not lines. For this reason, we prefer to have a numerical
system that is {\em less accurate}. In floating point, inaccuracy comes
from the fact that not all numbers are representable (due to finite
precision) and the result of an operation is always rounded to a
representable number. IEEE-754 floating point comes in different
``spice levels,'' with ``32-bits'' being ``float'' and ``64-bits''
being ``double.'' Although spice levels as low as 3 bits make
sense~\cite{murphy2019nan}, 8-bit (``mild'') is occasionally used in
real applications, and 16-bit (``half'') is quite common in machine
learning. Usually the reason to prefer half precision is that it uses
less memory, and so your GPU can store networks that are twice as big
in RAM. For this project we will also use half precision, and we will
be happy to save RAM, but more happy that its precision is low and so
it is practical (although silly) to achieve significant rounding
error. Another important reason to choose half precision is to make
the pun in the title.

A half precision float is 16 bits: One sign bit, five bits for the
exponent, and 10 bits for the mantissa. Like all IEEE-754 formats,
there is much more precision (more values are representable) near
zero. (Figure~\ref{fig:histogram}) Once you get to 1024,
only integers are representable. From 2048 to 4096, only even numbers
are representable. 65504 is the largest finite number, and up here,
only multiples of 32 are available.

\begin{figure}
FIGURE: HISTOGRAM
\caption{
Here I will put a histogram of the values.
  } \label{fig:histogram}
\end{figure}

% Where does floating point imprecision come from?
\paragraph{Origins of Imprecision.}
Floating point does have many perversions, but many programmers come
to believe all sorts of dangerous superstitions about it. One idea is
that floating point is somehow always inexact, and so that you always
have to check that two numbers are equal ``within some
epsilon''~\cite{murphy2014epsilon}. This may work ``in practice'' but
it is actually pretty sloppy. Floating point imprecision is not
random, nor is it constrained to a fixed epsilon. Operations are
defined much more usefully: Each one computes the mathematically
correct value, and then rounds (according to the ``rounding mode'') to
the nearest representable value. That's it. One consequence of this is
that you can get the exact result of 32-bit multiplication by doing
64-bit multiplication and then rounding to 32 bits. This also means
that the rounding error from a single operation can be as large as the
gap between representable numbers: Up to 32 for half-precision. But it
also means that operations whose results can be exactly represented
have no error; for example adding integral half values less than 512
will always give an exact integer result, which can be compared using
==. We will use this later in Section~\ref{sec:sixtyfive}. It is
neither necessary nor sufficient compare for ``equality'' with some
``epsilon.''

\paragraph{Rounding.}
IEEE-754 supports multiple rounding modes, like ``round-to-zero,'' and
``round-to-infinity'' (always round in the positive direction).
Throughout this paper we use ``round-to-nearest,'' which is also the
typical default (e.g.~for C++11 expressions evaluated at compile time,
it always uses round-to-nearest).\footnote{There is seldom reason to
  change the rounding mode, and since it is a stateful act, you're
  asking for it if you do. But the round-to-negative-infinity and
  round-to-positive-infinity modes are are useful for interval
  arithmetic, which is arguably the only truly reasonable way to use
  floating point. What you do is represent numbers as intervals (low
  and high endpoints) that contain the true value, and then perform
  each calculation on both endpoints. For computations on the low
  endpoint, you round down, and symmetrically for the high endpoint.
  This way, the true value is always within the interval, and you also
  know how much inaccuracy you have accumulated!} Similar results are
likely attainable for the other rounding modes, as well as
hypothetical rounding modes such as ``round away from nearest,'' but I
have not explored this.

\paragraph{Getting some nonlinearity.} \label{sec:plus128}
All transfer functions implemented with floating point have a finite
range. For our experiments with neural networks, we will focus on
transfer functions that map values in [-1, 1] to values in [-1, 1].
Almost half (48.4\%) of floating point values are in this interval and this
is a typical nominal range for activations in neural networks.

We only have two operations: Addition and scaling. Let's see what kind
of rounding error each of these gives us. First, addition. In order to
get a function that takes values in [-1, 1] to values in [-1, 1], we
want to first add a constant (giving us perhaps a large value) and
then add a negative constant, bringing us back in range. For example,
the constant 128 gives us the function

$$f(x) = x + 128.0 - 128.0$$

This is of course mathematically the same as $f(x) = x$ (the
identity), but with half precision we get a function that looks like
this

\begin{center}
\includegraphics[width=0.45 \linewidth]{plus128}
\end{center}


Between 128 and 256, only multiples of 0.125 are representable. So for
arguments in 0 to 1, the sum is rounded to one of the values 128.0,
128.125, 128.25, \ldots 129. From 64 to 128, multiples of .0625
($\sfrac{1}{16}\th$) are representable. So from -1 to 0, we get 127.0, 127.0625,
127.125, \ldots 128. Subtracting 128, all of the values are exactly
representable, giving us -1, -.9375, \ldots, -0.0625, 0, 0.125,
\ldots, 0.875, 1.

The result is a step function, but whose resolution is twice as high
for the negative range as the positive; had we added -128 and then
added 128, we would have seen the opposite bias in resolution. We can
easily see that this function is (computationally) ``non-linear''
despite being (mathematically) ``linear.'' This function is unlikely
to be a good transfer function, because it does not have a good
derivative: It's zero most places (flat segments) except at the
discontinuities, where it is undefined. We do test this approach
(with the constant 64.0) later, though.

Scaling gives similar results. Consider

$$f(x) = x * 100.0 * (1.0 / 100.0)$$

In this project we never actually divide (although this would not
violate linearity) since most floating point numbers have approximate
multiplicative inverses, and many are exact. We just compute the
reciprocal $\sfrac{1}{100} \approxeq 0.01000213623$ ahead of time and
multiply by that constant. Here's what that function looks like:

\begin{center}
\includegraphics[width=0.45 \linewidth]{times100zoom}
\end{center}

At this scale it appears linear, but it does have small imperfections
(see zoomed region). The function is symmetric about zero, since
multiplication will do the same thing to a positive number as it does
to its negative counterpart. Here, the roundoff error differs with the
magnitude. At inputs close to 1.0, the results of the first
multiplication must round to the nearest multiple of 0.0625 (as in the
additive example) but this error is scaled down by a factor of 100
when we multiply back to the [-1, 1] range. So it is almost invisible.
For inputs close to 0.0, the error approaches zero. The effect is complex
and depends on the constant we multiply by. For example, if we multiply
by a power of two, this only affects the exponent, and so the result is
exact.

\medskip
Is that it? Of course not! We can apply these operations in
combination, and many times, to create more interesting functions. The
best approach I found in this simple family is to repeatedly multiply
the input by a number very close to one. Here's what happens if you
multiply the input by $0.99951171875$ (which is the next number
smaller than one, equal to $1 - \sfrac{1}{2048}$) five hundred times,
and then scale back at the end:

\[
\begin{array}{rcl}
  f(x) & = & x * (1 - \sfrac{1}{2048}) * (1 - \sfrac{1}{2048}) * \ldots \textrm{500 times} \ldots * \\
  \, & \, & 1.3232421875$$
\end{array}
\]

\begin{center}
\includegraphics[width=0.85 \linewidth]{grad1}
\end{center}

I call this the \gradone\ function.

Multiplying $1.0$ by $(1 - \sfrac{1}{2048})$ five hundred times in
half precision yields $0.755859375$ (mathematically it would be $(1 -
\sfrac{1}{2048})^{500} = 0.78333$, so there is significant accumulated
error. We set $f(1.0) = 1.0$ by multiplying by the inverse of this
constant, which is $1.3232421875$.

Why does this result in the zig-zags? Multiplication by $(1 -
\sfrac{1}{2048})$ affects numbers differently. For constants less than
$6.1094760895 \times 10^{-5}$, the value is unchanged; we round back up to
the original value. For all other finite inputs it produces a smaller
value, but with rounding error that depends on the value. This error
accumulates and becomes significant with many iterations
(Figure~\ref{fig:rainbow}). Unlike the previous functions, the output
here is much smoother (it looks piecewise-linear); in each of these
segments its derivative is nondegenerate. Of course, this function is
mathematically linear. It is equivalent to $f(x) = x \times 1.036535$.

\begin{figure}[htp]
  \includegraphics[width=0.95 \linewidth]{rainbow} \\[1em]
  \includegraphics[width=0.95 \linewidth]{rainbow-error}
  \caption{
    How repeated multiplying by $1 - \sfrac{1}{2048}$ affects values in
    [0, 1].
    The width of the image is the interval [0, 1], with zero at the left.
    \\[1em]
    {\bf Top}: In the topmost row, we assign each pixel a hue so that
    we can track where those values go. For each pixel, we
    successively multiply by the constant and plot its color in its
    new $x$ position, the move to the next row down. Note that the
    rainbow shrinks exponentially as expected, but not smoothly.
    The black line is 500 iterations.
    \\[1em]
    {\bf Bottom}: The accumulated error when iteratively multiplying
    by the constant. Here the $x$ coordinate of the value does not move
    (so the middle column always represents the value that was
    originally 0.5). The color illustrates the accumulated error. For
    green pixels, the value is too high compared to the mathematically
    correct one; for magenta pixels too low. By choosing a row with
    alternations between green and red, we get the zig-zag pattern of
    the \gradone\ transfer function.
  } \label{fig:rainbow}
\end{figure}

So now we have a ``good'' candidate function, which we'll call \gradone.
It is ``good'' in the sense that it is computationally non-linear
despite being mathematically linear, so it may prove my professor
wrong. On the other hand, it requires 501 floating point
multiplications to compute, which is kind of slow. The ``good'' news
is that since there are only 65536 16-bit values, we can easily just
precompute any function for all possible half inputs, and store it in
a table of 131072 bytes. This allows us to execute the function
efficiently when performance is important, such as during training.
(Table lookup is certainly not a mathematically linear operation, so
when we require the computation to be linear for ideological purposes,
we can perform the 501 multiplications and get the same result.)

\paragraph{Differentiating.}
Speaking of training, in order to train a neural network using
stochastic gradient descent, we need to be able to evaluate the
derivative of the transfer function at any point.
 XXX TODO EXPLAIN IT

\subsection{Bonus digression: Downshift}
Having freed myself from needing to ``do math'' in order to
differentiate exotic functions, I pondered other weird transfer
functions. For example, the rectified linear transfer function is very
simple and works well, but is it the fastest possible transfer
function that might work? It does involve a conditional, which na\"ively
implies comparison and branching (although probably most processors
can do this with a conditional move). Because the floating point
format is packed with fields that represent different things, many
simple operations on its bits have interesting non-linear behavior.
The most promising I found was a right shift by two places.

\begin{center}
\includegraphics[width=0.95 \linewidth]{downshift2}
\end{center}

TODO: Diagrams of other operations

Shifting is about the cheapest possible thing a processor can do. Its behavior on floating point numbers is interesting,

DIAGRAM: TODO: in illustrator

\begin{verbatim}
[s][e][e][e][e][e][m][m][m] [m] [m] [m] [m] [m] [m] [m]
[0][0][s][e][e][e][e][m] [m] [m] [m] [m] [m] [m] [m]
\end{verbatim}

The sign bit is shifted into the exponent, which means that the output
is always non-negative (like the rectified linear function) and is
non-linear (discontinuity at zero, as negative numbers have a much
larger exponent that positive ones). Further nonlinearity comes from
the exponential representation (shifts divide the exponent by four)
and reinterpretation of exponent bits as mantissa bits. There is
additional weirdness in the details. Shifting by two places is better
than one, as it cannot produce Inf or NaN.
% Something about its derivative?
We will also evaluate this transfer function, called \downshifttwo,
below.

I implemented all this as a modification of my custom NN training and
inference system, ``Tom7Flow.'' Tom7Flow is generally much worse than
mainstream packages; it is based on deprecated OpenCL technology, is
prone to divergence or stagnation during training due to na\"ive
choices of hyperparameters, etc. But it is at least well suited to
silly experiments that take the form, ``What if deep learning but
worse?'' such as the current exercise. In order to realize the idea
completely, I modified the inference code to calculate with
half-precision arithmetic (not just the transfer function). This means
that the trained networks can be executed using only half-precision
operations (and just addition and multiplication by constants).
Unfortunately, while my GPU supports half-precision math natively, and
OpenCL supports half-precision operations as an
extension~\cite{openclextensions}, this extension is somehow not
supported (??) by my drivers, perhaps because OpenCL is so thoroughly
deprecated. It does support half precision as a *storage* format,
which allows you to write a full-precision float to a 16-bit value
(rounding to half) or read a 16-bit half into a float (all half values
can be represented exactly in full precision). So with this one
operation it is straightforward to implement half-precision addition
and scaling. You maintain the invariant that any float value is always
exactly a half, and after you perform addition or multiplication, you
round to half (by storing in a 16-bit memory location and reading it
back). This definitionally produces the same results as the native
operation.\footnote{I also verified consistent results using the
  \verb+half.h+ software implementation. Many of the evaluation
  results quoted in the paper are actually executed on the CPU
  using this library.}

I initially tried a version of training that worked entirely using
half precision (network parameters are half, backpropagated errors and
update values are half, etc.). This worked badly. It is ideologically
unnecessary, as we just care about producing a final model that,
during inference, only executes linear half-precision operations (but
abuses floating point roundoff to do something interesting.) This
network can be trained using non-linear techniques (and must anyway,
since for example its computed derivative is not linear). So during
training, calculations are done using full-precision floats, except
for the forward step (where we round to half after every operation).
In addition to being simpler, representing intermediate learned
weights as floats seems to help training approach the final half
values smoothly, avoiding stalls due to underflow.

TODO: half.h~\cite{half} somewhere above?

\subsection{Neural network experimental results}
In order to evaluate this transfer function, I ran a suite of
benchmark problems. For each problem, I compare the same network
architecture (i.e.~the number of layers, their connectivity, random
initialization, etc.) but using different transfer functions.

The transfer functions are:

\begin{itemize}
\item {\bf grad1}: The ``linear'' transfer function \gradone\ described above.
\item {\bf tanh}: The hyperbolic tangent function, which is a classic saturating (output is always in (-1, 1)) sigmoid.
\item {\bf logistic}: The function $1 / (1 + e^{-x})$, another classic sigmoid (but whose output is in (0, 1)). Each operation is performed with half precision.
\item {\bf leaky relu}: The rectified linear unit, but with a small slope below zero: \verb|x < 0.0 ? 0.1 * x : x|. This is the function I usually prefer in practice; its advantage over the standard relu is that it does not ``die'' (zero propagated error) when its input is negative.
\item {\bf downshift2}: Interpreting the half-precision input as a 16-bit word, right shift by 2 places, then reinterpret as half.
\item {\bf plus64}: $f(x) = x + 64 - 64$. This about the simplest function that has obvious rounding error. It only outputs 25 distinct values in [-1, 1] so its derivative is degenerate; I use its ``mathematical'' derivative $f'(x) = 1$.\footnote{Learning with this function might work better if we instead approximate the derivative by something non-constant, like by computing the derivative of a smoothed version. However, due to implementation tricks in Tom7Flow, we need a derivative that is expressed in terms of the transfer function's {\it output} (i.e.~$g(f(x))$ = $f'(x)$); we would not be able to express the smoothed derivative because there are only 25 distinct values of $f(x)$ in the [-1,1] range!}
\item {\bf identity}: The function $f(x) = x$. This is an important comparison because it shows us what a ``true'' linear (both mathematically and computationally) network is capable of.
\end{itemize}

TODO: Describe derivatives above?

\paragraph{Flattened models.}
For the transfer functions that are mathematically linear, we can also
compute the equivalent linear model. This just consists of a single
dense layer, using the identity transfer function, that computes the
linear function of the input. These appear in the results as ``flat''
variants.

\paragraph{MNIST.}
The first problem is the Modified National Institute of Standards and
Technology handwriting dataset (MNIST). This is a standardized dataset
of handwritten digits (0–9) as \xbyx{28}{28} greyscale images. This is chosen
partly for trollish reasons. It dates from 1998, and even at the time
of publication, accuracy with neural networks (98.4\%) and other
techniques (99.2\%) were already extremely high~\cite{lecun1998gradient}.

For this problem, I augmented the dataset by randomly offsetting the
training images by up to two pixels in any direction, and by adding
Gaussian noise. The model's input layer is just the $28 \times 28$
greyscale values, and the output is a prediction for each of the ten
digits. The models had two convolutional layers (64 \xbyx{3}{3} features,
fully overlapping + 128 \xbyx{8}{8} features, fully overlapping; then
32 \xbyx{128}{128} features + 32 \xbyx{256}{2} features with no
overlap), then two sparse layers of 1024 nodes each, then a final
dense output layer. The same initial weights and connectivity was used
for each experiment. Internal layers use the transfer function being
evaluated, but the output layer always used the identity transfer
function. This is not a good choice for this problem (softmax makes
more sense since the output is categorical) but I wanted the linear
models to be truly linear. Using the same transfer function would have
also disadvantaged functions with limited output range; \downshifttwo\
for example can technically output 1.0, but only for very large inputs
(8192.0). The final identity layer can easily scale the useful range
of the transfer function to the nominal range of the output. (This is
essential for the chess problem below, where the output instead ranges
[-1, 1].)

See the source code for various hyperparameter settings (although if
you are trying to learn good settings for hyperparameters, my code is
not the place to look). I used the ADAM weight update
trick~\cite{kingma2014adam}, which does give me much better results
than plain SGD in my experiments.

\begin{figure}[htp]
  \input{mnist}
  \caption{
    Results on the standardized MNIST data set. Accuracy is the
    fraction of results from the held-out test data for which the
    highest-scoring class (digit) is the correct class.
  } \label{fig:mnistresults}
\end{figure}

\begin{figure}[htp]
  \includegraphics[width=0.9 \linewidth]{mnist-bug}
  \caption{
    Bug.
  } \label{fig:mnistbug}
\end{figure}

Results for MNIST are in Figure~\ref{fig:mnistresults}.
A nice bug appears in Figure~\ref{fig:mnistbug}.

\paragraph{CIFAR-10.}
Another classic dataset comes from the Canadian Institute For Advanced
Research. They capitalize ``For'' so that the acronym can be
pronounced nicely. I mean to be fair MNIOSAT would have a certain ring
to it too. This dataset contains 60,000 RGB images of size $32 \times 32$, that
are labeled into 10 classes: Airplanes, cars, birds, cats, deer, dogs,
frogs, horses, ships, and trucks~\cite{krizhevsky2009learning}. It is
very similar to the handwriting problem but more challenging (state of
the art accuracy is more like 96.5\%). Like with MNIST, I augmented the
training set by randomly shifting the images and adding Gaussian
noise. The network structure is the same as in the MNIST problem,
except that in the first convolutional layer, each window is three
times as wide to account for the three color channels.

\begin{figure}[htp]
  \input{cifar10}
  \caption{
    Results on the standardized CIFAR10 data set. As with MNIST,
    accuracy is the fraction of results from the held-out test
    data for whom the highest-scoring class is the correct class.
  } \label{fig:cifar10results}
\end{figure}


Results for CIFAR-10 appear in Figure~\ref{fig:cifar10results}.

\paragraph{Chess.}
This problem attempts to learn a good evaluation function for chess
boards. Training examples are real chess positions (from the Lichess
database) evaluated by a strong chess engine
(Stockfish~\cite{stockfish}). Stockfish generates two classes of
scores: ``Mate in N'' if one side is known to have a series of N moves
that wins (but ``Mate in 1'' is still better than ``Mate in 4''), or a
more subjective score, measured in pawns. (The score in pawns can
seemingly be arbitrarily high, which is kind of funny because how
could you even have that many pawns on a 64-square board? Here's an
idea for a SIGBOVIK paper: What's the highest scoring chess position,
according to Stockfish, for which it cannot deduce mate?)
% r1r1r2k/p1p1p1p1/PpPpPpPp/1P1P1P1P/BRBRBRBR/RBRBRBRB/BRBRBRBR/K1RBRBRQ w - - 0 1
% Evaluated as +99.0 at depth 56/99
Mate is of
course categorically better than the pawn score, as it is exact.
Anyway, I squash this score into the range [-1, 1] and that becomes
the training instance. This network's first layer has 256 \xbyx{3}{3}
convolutional features, overlapping, as well as 32 \xbyx{1}{1} and
128 \xbyx{8}{1} and \xbyx{1}{8}. Each of these is measured in terms
of squares on the board, but each square actually corresponds to 13
inputs, for the 13 possible things that can be in that square (exactly
one set to 1.0). We also have some non-square inputs, like the
castling privileges and en passant state. So it's not just the
convolutional features but some sparse nodes too. And then we have
some more layers (you can check out the source code if you really care
about these details, which I doubt!) and then a final dense layer with
a single output using the identity transfer function as before. No
training data augmentation here, but I do normalize the board so that
it is always white to move.

For chess we can compute the accuracy, comparing to Stockfish as
ground truth (Figure~\ref{fig:chessresults}). We can also use the
evaluation function to play chess. These chess ``engines'' just look
at the possible legal moves and take the move that is most favorable,
using the learned evaluation function (no game tree search). Playing
against the best of these (``leaky'') it subjectively makes decent
moves most of the time and can even beat me playing casually. I
noticed that it had a lot of trouble ``sealing the deal'' in totally
winning positions (which is not unusual for engines that don't do
game-tree search or use endgame tables), but the problem was actually
more shallow: Due to a bug\footnote{ I used the annotations like
  \verb|[%eval #12]| that appear on moves for many games in the
  Lichess database. I didn't notice that they do not appear on a
  game-ending move like \verb+Qh4#+! This does sort of make sense
  because the eval scores would have to be \verb|[%eval #+0]| (``mate
  in 0'') or \verb|[%eval #-0]| (necessitating use of the floating
  point coprocessor) to express the winner, and there does not seem to
  be a natural way to express the definite value of stalemate. } in the
way training examples are gathered, the models were never exposed to
checkmate or stalemate positions! Since training takes several days
per function and the iron-fistedly punctilious SIGBOVIK deadlines were
imminent, there simply wasn't enough time to retrain them with access
to these positions. However, since mate is a mechanical fact of the
game (like what moves are legal) it seemed reasonable to fix this in
the engine itself: When considering all the legal moves to make, it
infinitely prefers a move that results in checkmate, and considers a
move resulting in stalemate to have score 0.0, and otherwise uses the
evaluation function. These ``fix'' versions of each engine perform
very significantly better, although they likely overestimate the
performance we'd get by actually fixing the model; there's no
guarantee that it would be able to accurately recognize mate, and the
fixed versions' greedy strategy of taking mate in 1 is always
advantageous.

These players compete against each other as well as the engines from
the Elo World project~\cite{murphy2019eloworld}, giving a sense of
their strength in an absolute scale (Figure~\ref{fig:elo}). The raw
versions perform reasonably; they all work better than a simple engine
like ``take the move that minimizes the number of moves the opponent
will have,'' (\verb+min_oppt_moves+). The fixed versions are much
better, as expected. The ``linear'' engine using the
\gradone\ transfer function, is competitive with the NES Chessmaster
engine, and outperforms a 50\% dilution of Stockfish. This is pretty
solid given that it is doing no explicit game tree search. In fact
(aside from the wrapper implementing the rules of chess and finding
the maximum eval score), it is only performing a fixed expression of
floating point addition and scaling! We could make this even more
ideologically pure using techniques from Section~\ref{sec:sixtyfive}.

\begin{figure}[htp]
  \input{chess}
  \caption{
    Results of learning Stockfish's position evaluation function.
    Stockfish scores are normalized to a [-1, 1] scale, and loss
    here is the average (L1) distance between the predicted score
    and actual Stockfish score, on some 100,000 positions from
    games not in the training set. Accuracy is the percentage of
    predictions whose sign agreed with Stockfish (e.g.~they both
    agree the white player is winning).
  } \label{fig:chessresults}
\end{figure}

\begin{figure}[htp]
  % This is the output of elo.exe but I manually inverted the
  % background colors to make it print better.
  \includegraphics[width=\linewidth]{elo}
  \caption{ Results of a chess tournament. Players include ones based
    on the learned position evaluation with different transfer
    functions; these players simply take the move that results in the
    most favorable eval (no game tree search). They compete with some
    standardized players from the Elo~World
    project~\cite{murphy2019eloworld}. Rows represent the player as
    white, columns as black. A green cell means that White generally
    wins; blue a draw; red a loss. An $\times$ in a cell means that
    this outcome occurred in every game. The left column is the
    resulting Elo rating~\cite{elo1978rating}. The best model
    {\tt leaky\_fix} performs decently well, similar to NES Chessmaster
    or a Stockfish diluted to about $60\%$ strength with random
    moves (both of these engines perform game tree search). The
    centerpiece of the paper is the ``linear'' \gradone\ transfer
    function; here its learned chess player slightly outperforms
    Stockfish diluted to $50\%$ strength with random moves.
  } \label{fig:elo}
\end{figure}

\paragraph{What transfer function is best?}
The results on each of these problems are similar: The ``leaky
rectified'' transfer function is generally best or close to best.
The identity transfer function, which yields a simple linear model,
is generally worst or close to worst. The sigmoid functions are
all over the place. The experimental \downshifttwo\ function is
generally bad, perhaps because its output is strictly positive or
it has such a small dynamic range. The small amount of nonlinearity
introduced by \plussixtyfour\ does seem to give it a small edge
over the identity, but its lack of an interesting derivative and
the fact that it only produces a small number of output values seem
to be limiting. Importantly, the \gradone\ function---the
centerpiece of the first third of this paper---performs decently on
all problems. It clearly outperforms the linear models, despite
being ``linear.''

It is also interesting to compare the flattened versions of the linear
transfer functions. These are the computed (mathematically) equivalent
single-layer linear models. For \plussixtyfour\ the flattened version
is worse in all cases; the unflattened model is taking advantage of
the discretization in some way. For \gradone\ it is dramatically
worse, both because \gradone\ models are substantially using the
roundoff error and because the mathematical version of this function
($f(x) = x \times 1.036535$) is not even a good linear approximation
of the actual result (e.g. $\gradone(1) = 1$). Finally, the result
for the identity transfer function should be mathematically equivalent,
but it does not always produce the same results. This is unsurprising
since we know that floating point calculations are not perfectly
accurate, but it does hint that deep networks may make use of
floating point roundoff internally, even if they are not using silly
transfer functions!

\smallskip
Having proved the professor wrong, we could stop there, but did huge
mathematical breakthroughs ever arise from taking the option to stop
there?!


\section{Non-monotonic functions}
Because of the way that addition and scaling are defined (do the real
mathematical operation, then round), they preserve monotonicity: If $x
\geq y$, then $f(x) \geq f(y)$. But this is only true if we limit the
form of the function to a series of additions and (positive) scaling.
There are other expressions that are mathematically linear but don't
take that form; for example:

% FIXME XXX TODO

$$f(x) = x + 0.25 - x$$

This is mathematically equivalent to $f(x) = 0.25$, but with floating point, it
is a boxcar function (CHECK)

% XXX this would instead be zigzags, right?
\begin{center}
\includegraphics[width=0.75 \linewidth]{boxcar25}
\end{center}

It takes the value 0.25 in places where there is enough precision to
represent $x + 0.25$, or 0 when $x + 0.25$ rounds to $x$.

Here is $f(x) = \gradone(x) - x$, which is also linear:
\begin{center}
\includegraphics[width=0.75 \linewidth]{grad1minusx}
\end{center}

Generally speaking, we can create a large variety of functions by
computing the interference patterns between other functions, since the
sum or difference of two ``linear'' functions is also ``linear.'' In
general we'll consider expressions of this form:

\[
\begin{array}{rrl}
E & ::= & x              \\
  &  |  & E * {\tt c}    \\
  &  |  & E + {\tt c}    \\
  &  |  & E + E          \\
\end{array}
\]

Where $x$ is the function variable, and {\tt c} is one of the 63,488
finite half-precision constants. We can derive negation $(E * -1)$ and
subtraction of constants $(E + -{\tt c})$ and expressions $(E + (E *
-1))$ since every number has an exact negation by flipping its sign
bit. Exact division is possible when $\sfrac{1}{{\tt c}}$ is
representable, and there is almost always a close approximation.

(XXX, can we get an animation approximating the sine function or something?)

This formulation leads to a tempting approach for approximating a
function iteratively, like a Taylor series. Given a target function
like $\mathrm{sin}(x)$, we can begin with an approximate expression for it,
like $x$, and then add and subtract terms to improve the
approximation. I don't know of any systematic way to improve the
approximation at each step (they are not well-behaved mathematically,
and I am not good at math), but by using computer search I can sure
make some complicated functions with many different shapes.

It is fun to watch an animation of the successively improving
approximations, but you can't see that since you're reading an
old-fashioned paper. Perhaps you can find a video of this at
\verb+tom7.org/grad+.

\subsection{Fractals} \label{sec:fractals}
Next, I endeavored to deploy these functions for something useful:
Fractals. Famously, fractals are simple functions with complex (often
literally) behavior. For example, the Mandelbrot set considers each
complex point $c$ (plotted on the plane as $x + yi$) and computes
whether $z_i$ = $z_{i-1}^2 + c$ diverges or not. It's lovely, but
squaring is not linear!

What if we just create a linear function that approximates $f(x) = x^2$?
This is definitely possible, using the approach described above. After
184 successful error-reducing rounds we get the following approximation,
with 112,204 linear operations:

\begin{center}
\includegraphics[width=0.55 \linewidth]{square}
\end{center}

Aside from the funny business near the origin, this is a fairly
accurate approximation of the square function, so you might hope that
it would draw a perverted Mandelbrot set. Unfortunately, it produces
a much sadder blotch (Figure~\ref{fig:squarebrot}). To see why, consider
the normal definition of squaring for a complex number:

\[
\begin{array}{rcl}
  (a + b\mathrm{i})^2 & = & a^2 + 2ab\mathrm{i} + b^2\mathrm{i}^2 \\
                      & = & a^2 + 2ab\mathrm{i} - b^2 \\
\end{array}
\]

Note that the real coefficient $a$ ends up part of the imaginary
coefficient $2ab$ in the result, and the imaginary coefficient $b$
becomes part of the real part (because $\mathrm{i}^2$ is real). This
means that squaring a complex number cross-pollinates between the two
components, yielding a kind of wacky rotation if we think of them as
2D coordinates.

But here, squaring is approximated as a series of operations
of the form $w_1 + w_2$ and $w \times c$ for constants $c$.
These operations on complex numbers are less interesting:

\[
\begin{array}{rcl}
  (a_1 + b_1\mathrm{i}) + (a_2 + b_2\mathrm{i}) & = & (a_1 + a_2) + (b_1 + b_2)\mathrm{i} \\[1em]
  (a + b\mathrm{i}) \times c & = & ac + bc\mathrm{i} \\
\end{array}
\]

Alas, these operations are boring; the real parts always stay real and
the imaginary parts always stay imaginary. This is why the crummy
blotch has all sorts of vertical and horizontal lines in it: As we
iterate the function we are iterating two independent components, and
the resulting picture is just some interference pattern between them.

This seems pretty definitive. Even if we had some kind of hardware
implementation of complex numbers with rounding error to abuse, there
would be no reason to have the linear operations do any
cross-pollinated rounding. Professors take note: The complex numbers
do provide some refuge!

\begin{figure}[htp]
  \includegraphics[width=0.95 \linewidth]{squarebrot}
  \caption{
    A garbage ``fractal'' that results from trying to approximate
    squaring of complex numbers using linear complex operations. Alas, it
    cannot be done. The complex numbers are truly special.
  } \label{fig:squarebrot}
\end{figure}

Still, a lot of chaos can emerge from these functions that should not
be possible with ``linear'' ones. For example, here is a complicated
function made by stringing 36,637 addition and scaling operations
together:

\begin{center}
\includegraphics[width=0.75 \linewidth]{perm16good2}
\end{center}

\begin{figure}[htp]
  \includegraphics[width=0.95 \linewidth]{perm16frac}
  \caption{
    A fractal made from iterating a ``linear'' function $f$. The color
    is the magnitude of $z_{256}$ with $z_i = f(z_{i-1}) \times c$.
    $c$ is the complex coordinate $x + y\mathrm{i}$, a constant.
  } \label{fig:perm16frac}
\end{figure}

Iterating this function produces chaotic results because of its
nonmonotonicity. In Figure~\ref{fig:perm16frac} I plot (using color)
the magnitude of $z$ after 256 iterations of

$$z_i = f(z_{i-1}) \times c$$

This is mathematically linear (as $c$ is a constant and $f$ a linear
function). Nonetheless, it produces an appealing picture. I think this
is a fractal in the sense that it is chaotic, has a color gradient,
and could be on the cover of an electronic music album. It is not a
fractal in the sense that if you zoom in on it, you get infinite
detail of self-similar shapes. In fact, if you zoom in on it only a
modest amount, you encounter rectangular pixels as you reach the
limits of half-precision floating point. (And because this fractal is
built by abusing those very limits, it is not even possible to get
more detail by increasing the accuracy!)
% what does it look like if rendered with floats instead?


\subsection{Bonus digression: Baffling numbers}
Imagine you are my professor. You assign a class project to ``make
fractals using floating point roundoff error,'' for some reason. You
spot me in the computer lab and I'm obviously way off track, because
on-screen is some kind of {\Large \it 3D fractal}. The Mandelbrot set cannot
be extended to three dimensions, you say, because of the Frobenius
theorem: Only algebras of dimension 1 (real numbers), 2 (complex
numbers) and 4 (quaternions) work~\cite{frobenius1878lineare}. Unclear
how the professor speaks the citation aloud in this scenario. I say I
``know'' this fact, but I ``don't care.'' You say that my
three-dimensional algebra can't be associative, because that's ``just
a mathematical fact.'' I say you know what else isn't associative?
{\em The floating point numbers, my dude}.

Enter the {\bf baffling numbers}, ill-advised extensions of the
complex numbers to three dimensions. Here we have numbers of the form
$a + b\mathrm{i} + c\mathrm{j}$. Addition is just pointwise, and there
are several options to complete the story for multiplication, namely
the values of the cells $A$, $B$, and $C$ in this table:
%
\[
\begin{array}{c|ccc}
  \multicolumn{1}{c}{\times} & 1          &  \mathrm{i} & \mathrm{j} \\
  \cline{2-4}
  1          & 1          &  \mathrm{i} & \mathrm{j} \\
  \mathrm{i} & \mathrm{i} & -1          & U \\
  \mathrm{j} & \mathrm{j} &  V          & W \\
\end{array}
\]

% What I actually used:

% AR, AI, AJ,
% BR, BI, BJ,
% CR, CI, CJ

% 0, 1, 0,   = i
% 0, 0, 1,   = j
% 1, 0, 0    = 1

The cells $U$, $V$, and $W$ are baffling numbers (i.e.~each some
$a + b\mathrm{i} + c\mathrm{j}$). Some choices are
degenerate, but this gives us a family of options. It is known that no
matter the choices, this does ``not work'' (in the sense that the
resulting algebra is not associative\footnote{Or else is equivalent to
  the complex numbers.}) but we don't need associativity to draw
fractals. Plus, who's gonna stop me, FROBENIUS??

\begin{figure}[htp]
  \includegraphics[width=0.95 \linewidth]{bafflebrot}
  \caption{
    TODO describe
  } \label{fig:bafflebrot}
\end{figure}

I tried a few options, but thought that $U = \textrm{i}$, $V =
\textrm{j}$ and $W = 1$ produced satisfyingly trippy results. The
Mandelbrot is straightforwardly generalized to the ``Bafflebrot'' (the
starting point $c$ is just a baffling number now; everything else is
the same). I generated a 3D object by defining an implicit surface
based on whether a sampled point is still inside the set after 25
iterations, using Marching Cubes~\cite{lorensen1987marching} to
discretize it. The object is truncated along its $\textrm{j}$ axis,
showing a perfect ripe Mandelbrot inside. The resulting mesh is 2
gigabytes and crashes every library that attempts to programmatically
simplify it. I do admire and encourage its defiant spirit. A
rendering appears in Figure~\ref{fig:bafflebrot}.

\medskip
Drawing fractals is fun and everything, but I grew weary of the
exercise because there is no real goal other than to make a cool
picture. Instead I turned to something with a clearer challenge to
overcome: Linear Cryptography.

\section{Linear cryptography} \label{sec:eightbitchoppy}
Cryptography is like fractals minus drugs. One of the most basic
components of cryptography is a pseudorandom number generator. This
kind of function takes some state and produces a new state that
``looks random.'' Given a pseudorandom number generator, we can
construct one-way functions (``hash functions'') and from those we can
make symmetric ciphers (using, say, a Feistel network), with which we
can encrypt and decrypt data.

Another thing that professors will tell you about cryptography is that
good cryptographic functions cannot be linear. (XXX an explanation of
what ``linear'' means in this context. Like XOR is linear, but we used
that as an example that has no linear approximation in the NN
section.) One good reason for this is that even if the function is
{\em a little bit linear} then linear cryptanalysis can be used to recover
bits of the key with a lot of example data. Standard advice is to
alternate both linear (e.g.~xor, or multiplication mod $2^n$) and
non-linear (e.g.~substitution) operations. (``[Substitutions] are
generally the only nonlinear step in an algoritm; they are what give a
block cipher its security.''\footnote{{\it Applied Cryptography},
  Second Edition, page~349~\cite{schneier1996applied}.} Of course we
will prove this wrong by developing a good pseudorandom function that
uses only ``linear'' operations on half-precision floating point
numbers.

$\uparrow$ XXX Here's a good description of linear cryptanalysis of DES: https://www.apprendre-en-ligne.net/crypto/bibliotheque/PDF/lincrypt.pdf

In terms of goals, pseudorandom number generation has a more clear
objective than fractals, although it's not so easy to pin down
formally. We don't even know if such functions exist, mathematically.
(XXX cite) There exist many functions that look like good pseudorandom
generators, but that actually have back doors that make them easy to
invert. (A symmetric encryption algorithm like AES, with the key
hidden, has this property.)\footnote{And let us never forget that RSA
  DSI (yes, that RSA) took a \$10 million bribe from the NSA to put a
  backdoor in one of their pseudorandom number
  generators~\cite{reuters2013secret}!} Practically speaking, though,
we can subject the function to a wide variety of statistical tests,
and if it looks random to every test, then this gives us good
confidence.\footnote{Truly good cryptographic algorithms are also
  openly studied by experts. Of course nothing in here is to be used
  seriously, and not just because these algorithms are ridiculously
  slow. But I guess if you are stuck on a desert island with only the
  floating point addition and scaling operations, and a copy of this
  paper, then it would be a reasonable starting point for encrypting
  your messages. I do not recommend, if stranded on a desert island,
  to send encrypted messages: They may not be readable to your
  potential rescuers!}

Specifically, my goal is to design an algorithm that takes 64 bits of
data (represented as half-precision floats) to another 64 bits, such
that the stream of low-order bits from iterating this function passes
the TestU01's ``Big Crush'' suite of 106 statistical
tests~\cite{lecuyer2007testu01}. This suite is a successor to
Marsaglia's ``DieHard'' battery of tests~\cite{marsaglia1996diehard},
itself an improvement on Knuth's tests from The Art Of Computer
Programming~\cite{knuth1997art}.

The basis of this function is the classic substitution-permutation
network. First, each of the eight bytes are substituted with a
different byte using a table (this is the mathematically non-linear
step). Then, the 64 bits are permuted. Finally, some of the bytes are
modified additively. An illustration appears in Figure~\ref{fig:cipher}.

\begin{figure}
\includegraphics[width=\linewidth]{cipher}
\caption{The substitution-permutation network that forms a half decent
  pseudorandom number generator. The same substitution (``s-box'') is
  applied to each byte. Then the 64 bits are permuted. Finally, bytes
  are combined with modular addition and subtraction. This function
  passes the ``Big Crush'' suite and can be implemented with only
  half-precision floating point addition and scaling.
} \label{fig:cipher}
\end{figure}

The substitution table (``s-boxes'') was generated by computer search
with an objective to maximize the ``avalanche property'' (when a bit
of the input is complemented, about half of the output bits should be
complemented). The permutation was generated to maximize dispersion;
each quartet sends each bit to a distinct quartet in the output. This
is not the important part. We could have just used known good tables.

To implement this with half-precision floating point, we could
represent each bit with its own half, but that is no fun. The state
will be represented with eight half-precision floats, each
representing one byte's worth of information. A byte will be stored as
a values in [-1, 1), with each $\sfrac{1}{128}$ interval representing
one of the 256 values (0 is anything in [-1, -0.9921875), 1 is
anything in [-0.9921875, -0.984375), and so on). This means that
iterating the function on {\em any starting value} in the [-1, 1]
interval will produce pseudorandom results. So for example
we can guarantee that a ``fractal'' plotted using this function
will look ``fully messed up'' and not just have a few
distinguished points of randomness.
% (redundant with below.)
% \!\footnote{In fact the chaotic
%  function used to make fractals in Section~\ref{sec:fractals} was
%  an early attempt to create a 4-bit permutation using the
%  iterative approximation approach. Although it gets reasonably
%  close to the goal, it does not work for cryptography because
%  iterating it tends to reach fixed points or short cycles.}
I'll say now that this is unnecessarily hard; in the next
section of this paper we'll see a vastly more efficient
approach for handling discrete data. But working on the entire domain
makes for some challenging problems and shows that we've developed
substantial mastery of the continuous case.

Speaking of which, my first approach was to try to approximate the
substitution function (since it replaces one 8-bit byte with another,
it corresponds to a single discontinuous function of type $half
\rightarrow half$) using the iterative approach described in
Section~\ref{sec:fractals}. Although it is possible to get reasonable
approximations with this method (most values are transformed to a
value near the desired one), this will not suffice; when iterating the
function we find that the value easily gets stuck in short cycles due
to this inaccuracy.

I found a better approach, by creating a composable family of functions
that isolate specific intervals of interest. For example,

% 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0
\includegraphics[width=0.95 \linewidth]{basisvec}

% XXX just fix it
% (This function is actually discontinuous.)

Within the interval [-1, 1), this function takes on exactly two
  values: zero\footnote{Actually, $-0$!} and $\sfrac{1}{128}$. It returns
  $\sfrac{1}{128}$ only for exactly the interval $[\sfrac{121}{128},
    \sfrac{122}{128})$. This is the interval that represents the
  number 249 ($128 + 121$; remember that the first 128 integers are in
  $[-1, 0)$). The expression that computes this is

\[
\begin{array}{l}
  f(x) = \\[0.5em]
  \left(
  \begin{array}{l}
    (x - \frac{9}{64} - \frac{1}{4} \times{} \frac{-1}{512} - \frac{1255}{512} \times^{\scriptscriptstyle 164} \frac{1027}{1024})\, + \\[0.5em]
    (x - \frac{1}{4} \times{} \frac{-1}{512} - \frac{597}{512} \times^{\scriptscriptstyle 188} \frac{1277}{2048} \times{} -\!1)\, + \\[0.5em]
    \frac{517}{128} \times{} -32 \\[0.5em]
  \end{array}
  \right) + \\[2em]
%
  \left(
  \begin{array}{l}
    (x - \frac{17}{128} - \frac{1}{4} \times{} \frac{-1}{512} - \frac{1255}{512} \times^{\scriptscriptstyle 164} \frac{1027}{1024})\, + \\[0.5em]
    (x - \frac{1}{4} \times{} \frac{-1}{512} - \frac{597}{512} \times^{\scriptscriptstyle 188} \frac{1277}{2048} \times{} -\!1)\, + \\[0.5em]
    \frac{517}{128} \times{} 32
  \end{array}
  \right) \times{} \frac{-1}{32} \\
\end{array}
\]

where $E \times^{n} c$ means $E \times c \times c \times c \ldots$ for $n$ iterations.

Mathematically this is equivalent to this constant function (all $x$s cancel out):
$$f(x) = \frac{13^{164} \times{} 79^{164}}{2^{1656}}$$

I spent a long time writing code to simplify these expressions and
generate \LaTeX\ for them, by the way! As usual, I thought it would
look cool when I got it working, but it just looks like a bunch of numbers.

We can think of this function as a basis vector, representing the 256-dimension
vector $\langle 0, 0, 0, \ldots, 0, 1, 0, 0, 0, 0, 0, 0 \rangle$. We'll call
this one $\basis_{249}$ since it selects the integer 249. If we can find $\basis_n$
for each $n \in \mathbb{Z}_{256}$, then we will be able to combine them
to systematically construct functions.

The one just pictured is one of the smallest expressions; most are
much larger. I wish I could tell you that I figured out the principles
underlying how to analytically generate these functions, but I
discovered them with computer search and some elbow grease.

\paragraph{Choppy functions.}
I call a function $f$ ``choppy'' if for every half precision floating point value in
$[-1, 1)$ it has the following properties:

\begin{itemize}
\item For $n \in \mathbb{Z}_{256}$ and $r \in [0, 1)$,
  $f(-1 + \sfrac{n + r}{128}) = v$ for the same value $v$. We only need to
  consider cases where $n + r$ is representable as a half.
\item $v$ is itself of the form $\sfrac{n - 128}{128}$ for some $n \in \mathbb{Z}_{256}$.
\item For these purposes, we treat the single value $-0$ as being equal to $0$.
\item And, as usual, the function is built only with floating point addition
  and scaling by constants.
\end{itemize}

That is, the function produces the same result for any representation
of an integer, and that result is the smallest representation of an
integer. These functions are maximally useful in that they are
``liberal in what they accept,'' but ``conservative in what they
return''~\cite{rfc761}. It also means that each function also can be
understood as a function $\mathbb{Z}_{256} \rightarrow
\mathbb{Z}_{256}$, so we can represent them as a vector of 256
integers. The basis vectors $\basis_n$ are those that are of the form
$\langle 0, \ldots, 1, \ldots, 0\rangle$.

I then conducted computer search for choppy functions, putting those
into a database (keyed by the corresponding integer vector). Some are
easy to find, others harder. Summing and scaling choppy functions
yield choppy functions (as long as the vectors remain integral and in
range), so I use a simplified version of Gauss-Jordan
elimination~\cite{gaussjordan} to solve for basis vectors. Once I have
$\basis_n$, this column can be changed at will for any existing choppy
function (by just adding or subtracting multiples of $\basis_n$), so
new choppy functions that only vary in that column can be ignored.

By trying a variety of operations that are known to be useful
(e.g.~iterated multiplication of constants near $1.0$) and
hill-climbing towards functions with the choppy property, it is not
too hard to find $\basis_n$ for most $n$. It seems to become more
challenging for $n$ near 128; this is the point $0.0$ in
half-precision. Specifically, the hardest problem was to make a
function that produced different results for inputs $< 0.0$ versus
inputs $\geq 0.0$. This is the {\em zero-threshold problem}.

% XXX something about optimization, which I wasted loads of time on?

\paragraph{Why is this hard?}
Distinguishing between negative and non-negative numbers is
deceptively difficult. Looking back to the function $f(x) = x + 128.0
- 128.0$ (Section~\ref{sec:plus128}), it has useful discontinuous
steps, but note that the discontinuity does not happen at zero. This
is because we are rounding to the nearest value, and so small negative
numbers near zero end up rounding to the same result that zero does.
Moreover, the resolution of the floating point numbers is highest near
zero (especially because of subnormal numbers), which exacerbates our
attempts to control rounding of them. For example, you might think
that we could simply shift this function left and right by
substituting $x + c$ for $x$ in its body. This would work
mathematically, but it does {\em not} work for floating point numbers,
because each operation performs some rounding. If this rounding ever
ends up conflating a negative number with a non-negative one, we will
not be able to recover.
% (But zero-threshold does start with adding. Why does it work?)

I found a zero-threshold function using a combination of manual and
computer search. This was some ordeal, and the resulting enormous
function is in Figure~\ref{fig:zerothreshold}. Perhaps you are
smarter than me and can find a better one!

\begin{figure*}[tp]
$zt(x) = x + \frac{1}{2^{24}} \times^{\scriptscriptstyle 9743} \frac{1025}{1024} \times{} \frac{39}{2^{19}} + \frac{1279}{16384} \times{} 4100 + 318 \times{} 4 + 1 \times^{\scriptscriptstyle 559} \frac{1025}{1024} \times{} \frac{1027}{2048} \times^{\scriptscriptstyle 1160} \frac{1025}{1024} \times{} \frac{545}{2048} \times^{\scriptscriptstyle 23} \frac{1025}{1024} \times{} \frac{311}{512} \times^{\scriptscriptstyle 137} \frac{1025}{1024} \times{} \frac{527}{2048} \times^{\scriptscriptstyle 365} \frac{1025}{1024} \times{} \frac{627}{1024} \times^{\scriptscriptstyle 346} \frac{1025}{1024} \times{} \frac{593}{1024} \times^{\scriptscriptstyle 676} \frac{1025}{1024} \times{} \frac{281}{512} \times^{\scriptscriptstyle 557} \frac{1025}{1024} \times{} \frac{129}{256} \times^{\scriptscriptstyle 830} \frac{1025}{1024} \times{} \frac{589}{4096} \times^{\scriptscriptstyle 336} \frac{1025}{1024} \times{} \frac{1029}{2048} \times^{\scriptscriptstyle 663} \frac{1025}{1024} \times{} \frac{1041}{2^{18}} - 2206 + 2206 \times{} \frac{9}{256} + 1076 + 2534 - 2074 \times{} \frac{17}{64} - 2048 + 2048 \times{} \frac{1}{8} $ \\

\scalebox{2}{$$=$$}
\[
\begin{array}{l}
\frac{593 \times{} 2187 \times{} 5^{30793} \times{} 343 \times{} 11 \times{} 169 \times{} 289 \times{} 361 \times{} 961 \times{} 41^{15396} \times{} 43 \times{} 79 \times{} 109 \times{} 281 \times{} 311 \times{} 347}{2^{154102}}\scalebox{2}{$\,x\,+$} \\[1em]
\frac{\begin{array}{r@{}l}
% XXX: Maybe could fit more of it with a small font?
\, & 84107227537103367748705454682078539303191039250504832410385579895977259206417678 \\
\, & 54245560547677512932803511681703341508665266147325419821191498222214690318490003 \\
\, & 88110778422408950986678118366271064412982414738166383752334216371785576710459496 \\
\, & 25738831829937485787963475192987844674323695457715738688221958462470493961327089 \\
\, & 64862528034085403084792949523917534005532171250047637347672007635216035917044700 \\
\, & \ldots 569\ \mathrm{lines} \ldots \\
\, & 73014930689000221762919045089322540125964944324282583780813532524840229888776299 \\
\, & 45268638388603643723804098205965854510116202420980541689175292145265852612920173 \\
\, & 68725838005728517370192463512280524432138902703991548800398876262333592383735651 \\
\, & 92764023532792804235216160403774463302046032255421688296905932246680375562746379 \\
\, & 76285400623707503241570527062342946483273904883074176883724035214883942317941586 \\
\, & 85212883934224294828781615577153021816836375301250331354703790141535016001286984 \\
\, & 69246587905140878604602957637178398679300714629238352320436905873649993209 \times{} 3 \times{} 17 \times{} 421 \\
\end{array}}{2^{154126}} \\
\end{array}
\]
\caption{
  A zero-threshold function. Returns $\sfrac{1}{128}$ for values in $[0, 1)$ (and $-0$) and $0$ for values in $[-1, 0)$.
      Top is the series of additions and scalings to perform, all from left to right. At bottom
      is the equivalent mathematical expression, but the enormous numerator cannot be printed due to
      extremely oppressive SIGBOVIK page limitations.
  } \label{fig:zerothreshold}
\end{figure*}

\paragraph{Substituting and permuting.}
In any case, with this function it was possible to form a complete basis. This basis
makes it ``easy'' to perform operations on half values that represent bytes. For
example, the s-box step substitutes some distinct byte for each different input
byte. This would normally be implemented with a table lookup. If we compute
$\basis_n(x) \times \mathrm{subst}[n]$, this returns the correct\footnote{Technically
  we need to do some multiplicative adjustments to put the value in $[-1, 1)$.}
result $\mathrm{subst}[n]$ if the input $x = n$, and $0$ otherwise.
So if we just sum all 256 of these up, exactly one of them will be nonzero,
and the correct substituted value.

Permutation is defined on the component bits. Here, we compose a function that
computes each of the eight output bytes. We use the same approach of summing
a bunch of $\basis_n(x)$ evaluations (each multiplied by the correct answer).
Here we are testing whether the input has some particular bit set (a sum of the
128 $\basis_n(x)$ functions where $n$ has that bit set), and the output is the
power of two that sets the appropriate output bit. Many of these functions would
have simpler implementations (for example, ``is the high-order bit set?'' is the
same as the zero-threshold function) but at this point I was happy to just have
something working, and taking some joy in how absurdly large the functions
were getting.

The cipher also includes addition and subtraction mod 256. Addition
and subtraction are already available for half-precision floats, and
they have faithful behavior, so we just need to implement the wrapping-around
behavior so that the result is strictly in $[-1, 1]$.
This is straightforward with the zero threshold function;\footnote{Compare to
  the remarks ``why is this hard?'' above. Here, $zt(x - 1)$ {\em does} do what you'd want,
  shifting the threshold value from $0$ to $1$. This is because there is {\em more}
  precision near zero than near one.}
we produce a corrective factor if the result is $\geq 1$ or $< -1$ (and zero otherwise).
We then add those corrective factors produce the remainder we desire.

TODO: nextafter. ``floating point is like JavaScript. Every time you get seriously into it, you will discover some horror that you had never known before.''

\paragraph{Benchmark results.}
To evaluate the quality of the pseudorandom number generator, I used
the TestU01 ``Big Crush'' suite. This test needs a sample of 1.64
billion bits, so I actually evaluated it on equivalent code that
performs the steps using normal integer operations. Even then, the
suite takes several days to run, so I modified it to run tests in
parallel and cache the results of completed tests. This saved me from
losing data if my computer crashed or needed to be rebooted.

Results appear in Figure~\ref{fig:testu01}. Passing these tests does
not ensure that the pseudorandom number generator is good for
cryptography, although it is a good start.

\begin{figure*}[tp]
  \footnotesize
  \begin{tabular}{rl|rl|rl}
    {\bf Test} & {\bf p-value} &
    {\bf Test} & {\bf p-value} &
    {\bf Test} & {\bf p-value} \\
    \input{testu01}
  \end{tabular}
  \caption{
    Results of the TestU01 ``Big Crush'' suite on the
    pseudorandom number generator built from floating point roundoff
    error. A p-value of $< 0.001$ or $> 0.999$ is considered
    suspect by the suite, so all tests pass here.
  } \label{fig:testu01}
\end{figure*}

Running single-threaded on a 3.0 GHz Threadripper 2990WX, this
function generates 25.8 bytes of randomness per second, which is slow.
By precomputing the substitution, permutation, and zero threshold
expressions (so they can be performed by lookup into 64k-entry tables),
it generates 18,685.2 bytes per second, which is still slow.

If we were building an encryption algorithm (a symmetric block
cipher), it would be natural to use this as its ``round function.'' In
a Feistel network~\cite{feistel1973cryptography}, each input block
(128 bits) is broken into two halves; one of them is mixed with some
key bits (for example with XOR) and then passed to this function. Its
output is XORed with the other half; the two halves are swapped, and
this ``round'' is repeated many times until we believe that the data
are suitably screwed up. Decryption is the reverse. We can use
addition and subtraction mod $2^8$ to combine the data instead of XOR
(which is addition mod $2^1$), so we already have all the operations
we need to build a whole block cipher here. As Bruce Schneier
says,\footnote{{\it Applied Cryptography}, Second Edition, page 351.}
``It is easy to design a block cipher.''


\section{Non-linear gameplay} \label{sec:sixtyfive}
Having developed a basis for extracting arbitrary bits, we can express
any function of a single variable, and we've seen how some other
functions (like addition mod $2^8$) can be done. At this point, it
seems like we probably have the building blocks to demonstrate that
addition and scaling on half-precision floats is Turing complete. I
mean, pretty much everything is Turing complete. In the past, I built
computers that were perfect and beautiful, such as a hardware
implementation of the NaNDY 1000, a computer architecture that
computes using only floating point NaN and
Infinity~\cite{murphy2019nan}. In a concession to ideological purity,
though, the NaNDY 1000 has no I/O. So it is very boring to use.

For today's investigations of the capabilities of floating point, I'll
make the opposite concession: Let's make a computer that is exciting
to use, but that makes some (reasonable) ideological concessions so that
it can do something interesting.

\subsection{Fluint8}

First of all, if we want to do some serious computation, 25.8 bytes
per second isn't going to cut it. To look for performance enhancing
substances, I perused the back catalog of the world's most prestigious
conference, SIGBOVIK. There in the 2018 edition, on page 125, I found
an intriguing paper, {\it The fluint8 Software Integer Library}, by
Jim McCann and \ldots Tom Murphy VII? Wait, that's me? {\large \em I already
wrote this paper?!}

The {\it fluint8} library represents an element of $\mathbb{Z}_{256}$
(a.k.a.~{\tt uint8}) as a 32-bit float, and provides multiplication,
addition, subtraction, negation, division, and bitwise functions and,
or, and exclusive or.

Compared to the approach discussed in Section~\ref{sec:eightbitchoppy}
using ``choppy functions,'' {\it fluint8} has much more simple and
sensible implementations of functions like addition:

\begin{lstlisting}
inline float fu8_add(float a, float b) {
  float x = a + b;
  x -= x - 127.5f + 3221225472.0f - 3221225472.0f;
  return x;
}
\end{lstlisting}

The \verb|x -= x...| line applies the corrective factor to implement
wrap-around, which we previously did using the zero-threshold function.
Why can it be done so much more simply here? First, {\it fluint8}
represents $n \in \mathbb{Z}_{256}$ as $n$, so a number like $27$ is
represented as $27.0$ instead of $-1 + \sfrac{27}{128}$.
%  This is
% somewhat less fiddly, since all numbers are positive and roundoff
% error is easier to come by as values get larger.
Second, it
requires that the number be represented {\em exactly} as this value.
The figures in the paper are somewhat misleading as they are plotted
only for input values that are already exact integers; if we test
\verb+fu8_add+ on values like 100.1875 and 11.0703125 we do not get
111 (Figure~\ref{fig:adderrordetail}). On the other hand, this is a
very reasonable choice to make; we can simply have a representation
invariant that only one of these 256 values is used, and preserve that
invariant with every operation. It won't work great for the continuous
domain (e.g.~plotting fractals) but is a much better choice for
discrete data (e.g.~encryption).

\begin{figure}[htp]
  \begin{center}
    \includegraphics[width=0.75 \linewidth]{add-error-detail}
  \end{center}
  \caption { Error of the {\it fluint8} addition function on general
    floating point values in [0, 256). This is a detailed zoom of the
      region $x \in [252, 256)$ and $y \in [0, 4)$, but the rest of
          the image is almost identical. Each pixel compares the sum
          of $x$ and $y$ to the expected value ($\lfloor x \rfloor +
          \lfloor y \rfloor \,\textsf{mod}\, 256$). The top-left pixel in
          each cell is the case where $x$ and $y$ are integers; we get
          the correct result (no error). All other pixels are wrong,
          either too high (green) or too low (red).
          %
          Graphic produced using {\tt ImageRGBA} computational
          {\tt for} loop engine.
  } \label{fig:adderrordetail}
\end{figure}

This same approach will work with half-precision floats, as every
number in [0, 256) can be represented exactly.\footnote{In fact, all
    integers from -2048 to 2048 are available, so we could consider
    implementing signed 11-bit numbers in a future {\it flsint11}
    library.}

Finally, the library uses several operations that are not linear!
In particular, its implementation of bitwise functions like xor
perform squaring and multiplication of the two arguments. It
was not a design goal of {\it fluint8} to use only addition and
scaling, but it is a design goal today, so we must address that.

\subsection{hfluint8}

half float, hf, l stands for ``linear'' this time

\comment{ OK! Summary to self for later: The tricks in the fluint8
  paper work for integral values, just like my fluint8.h. The graphs
  are a bit misleading in this sense. In particular (see old.cc), the
  endpoints of the intervals for the fmod function are

\begin{lstlisting}
 c3800000 -> c3800000 (-256 -> -256)
 bf000080 -> 00000000 (-0.50000762939 -> 0)
 437f8001 -> 43800000 (255.50001526 -> 256)
 43ffc000 -> 44000000 (511.5 -> 512)
\end{lstlisting}

So it works great for its purpose, but not if we insist on intervals like [0, 0.125).
}

\subsection{It's a-fine, Mario!}

\begin{figure}[htp]
  \begin{center}
    \includegraphics[width=0.9 \linewidth]{testrom}
  \end{center}
  \caption{
    Exciting ``game''
  } \label{fig:testrom}
\end{figure}

This also gives us an even more linear chess engine (including
game tree search and a user interface) by emulating {\tt chessmaster.nes}.


% For posterity, old approaches to IsntZero
\begin{lstlisting}[language=C]
Fluint8 Fluint8::IsntZero(Fluint8 a) {
  // A simple way to do this is to extract all the (negated) bits
  // and multiply them together. But this would mean multiplying
  // a by itself, and so is not linear.
  //
  // Instead, do the same but ADD the bits. Now we have a
  // number in [0, 8]. So now we can do the whole thing again
  // and get a number in [0, 4], etc.

  half num_ones = (half)0.0f;
  Fluint8 aa = a;
  for (int bit_idx = 0; bit_idx < 8; bit_idx++) {
    Fluint8 aashift = RightShift1(aa);
    half a_bit = aa.h - LeftShift1Under128(aashift).h;
    num_ones += a_bit;
    aa = aashift;
  }

  ECHECK(num_ones >= (half)0.0f && num_ones <= (half)8.0f);

  // now count the ones in num_ones.
  aa = Fluint8(num_ones);
  num_ones = (half)0.0f;
  for (int bit_idx = 0; bit_idx < 4; bit_idx++) {
    Fluint8 aashift = RightShift1(aa);
    half a_bit = aa.h - LeftShift1Under128(aashift).h;
    num_ones += a_bit;
    aa = aashift;
  }

  ECHECK(num_ones >= (half)0.0f && num_ones <= (half)4.0f);

  // and again ...
  aa = Fluint8(num_ones);
  num_ones = (half)0.0f;
  for (int bit_idx = 0; bit_idx < 3; bit_idx++) {
    Fluint8 aashift = RightShift1(aa);
    half a_bit = aa.h - LeftShift1Under128(aashift).h;
    num_ones += a_bit;
    aa = aashift;
  }

  ECHECK(num_ones >= (half)0.0f && num_ones <= (half)3.0f);

  // and again ...
  aa = Fluint8(num_ones);
  num_ones = (half)0.0f;
  for (int bit_idx = 0; bit_idx < 2; bit_idx++) {
    Fluint8 aashift = RightShift1(aa);
    half a_bit = aa.h - LeftShift1Under128(aashift).h;
    num_ones += a_bit;
    aa = aashift;
  }

  ECHECK(num_ones >= (half)0.0f && num_ones <= (half)2.0f);

  // Now num_ones is either 0, 1, or 2. Since 1 and 2 is each
  // represented with a single bit, we can collapse them with
  // a shift and add:
  //   num_ones    output
  //         00         0
  //         01         1
  //         10         1
  Fluint8 nn(num_ones);
  return Fluint8(nn.h + RightShift1(nn).h);
}

static Fluint8 OldIsZero1(Fluint8 a) {
  // We know IsZero returns 1 or 0.
  // return Fluint8(1.0_h - IsntZero(a).h);

  Fluint8 aa = a;
  // true if everything is 1.0 so far
  half res = 1.0_h;
  for (int bit_idx = 0; bit_idx < 8; bit_idx++) {
    // leftmost bit
    half bit = RightShift<7>(aa).h;
    half nbit = (1.0_h - bit);
    // res = res & ~bit
    res = RightShiftHalf1(nbit + res);
    // aa = LeftShift<1>(aa);
    // We already have the high bit, so mask it off if necessary
    aa = Fluint8(aa.h - bit * 128.0_h);
    aa = LeftShift1Under128(aa);
  }

  return Fluint8(res);
}
\end{lstlisting}


% old approach to If

\begin{lstlisting}[language=C]
  // For cc = 0x01 or 0x00 (only), returns c ? t : 0.
Fluint8 Fluint8::If(Fluint8 cc, Fluint8 t) {
  // Multiplying cc * t gives us what we want, but this may
  // violate linearity (e.g.~if the two inputs are the same).

  // Could do this by spreading the cc to 0xFF or 0x00 (cc * 255 will
  // do that) and using AND, but it is faster to just keep consulting
  // the ones place of cc.

  half kept = GetHalf(0x0000);
  Fluint8 tt = t;
  for (int bit_idx = 0; bit_idx < 8; bit_idx++) {
    // Low order bit as a - ((a >> 1) << 1)
    Fluint8 ttshift = RightShift1(tt);
    half t_bit = tt.h - LeftShift1Under128(ttshift).h;
    // Computes 2^bit_idx
    const half scale = GetHalf(0x3c00 + 0x400 * bit_idx);

    const half and_bits = RightShiftHalf1(t_bit + cc.h);
    kept += scale * and_bits;

    // and keep shifting down
    tt = ttshift;
  }

  return Fluint8(kept);
}
\end{lstlisting}

% Goedelization? ??

\section{Conclusion}
The professor has been defeated. The dead horse has been beaten.

\bibliography{grad}{}
\bibliographystyle{plain}

\end{document}
